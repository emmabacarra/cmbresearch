{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <b>IBM - What is an Autoencoder?</b>\n",
    "\n",
    "https://www.ibm.com/topics/autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Overview</u>\n",
    "\n",
    "- within the category of encoder/decoder architecture\n",
    "    - variational autoencoders (VAEs)\n",
    "    - adversarial autoencoders (AAEs)\n",
    "- useful for \n",
    "    - future data extraction (facial recognition, data compression, etc)\n",
    "    - generative tasks (image generation or generating time series data)\n",
    "- <mark>designed to identify <i><b>latent variables</b></i>, or inferred pieces of information based on common factors between data</mark>\n",
    "    - not directly observable\n",
    "    - impact the interpretations of how data is fundamentally distributed\n",
    "    - exists in a <i>latent space</i> (i.e. the collection of these variables)\n",
    "    - represents the most important pieces of information from the data\n",
    "- <mark>uses <b>unsupervised</b> machine learning</mark>\n",
    "    - the model learns which latent variables are the most important\n",
    "    - then uses that information to try and <b>reconstruct original data</b>\n",
    "    - don't rely on labeled training data or comparing with a ground truth (supervised)\n",
    "    - the model compares its outputs against the original input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>How it Works</u>\n",
    "\n",
    "Encoder $\\rarr$ Bottleneck $\\rarr$ Decoder\n",
    "\n",
    "**Autoencoder Structure**\n",
    "- Encoder compresses input data through dimensionality reduction.\n",
    "- Bottleneck contains the most compressed representation of input.\n",
    "- Decoder reconstructs data back to its original form.\n",
    "\n",
    "**Purpose of Autoencoders**\n",
    "- Discovering minimum important features for effective reconstruction.\n",
    "- Reconstruction error measures the efficacy of the autoencoder.\n",
    "\n",
    "**Role of Decoder**\n",
    "- In some cases, discarded after training to train the encoder.\n",
    "- In VAEs, decoder outputs new data samples post-training.\n",
    "\n",
    "**Advantages of Autoencoders**\n",
    "- Capture complex non-linear correlations.\n",
    "- Use non-linear activation functions like sigmoid.\n",
    "\n",
    "**Adaptations and Hyperparameters**\n",
    "- Code size determines data compression.\n",
    "- Number of layers affects complexity vs. processing speed.\n",
    "- Number of nodes per layer varies based on data nature.\n",
    "- Loss function optimizes model weights during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Use Cases</u>\n",
    "\n",
    "**Data Compression:** Autoencoders naturally learn compressed representation of input data.\n",
    "\n",
    "**Dimensionality Reduction:**\n",
    "  - Encodings learned by autoencoders can be used in larger neural networks.\n",
    "  - Reducing complexity can extract relevant features for other tasks and increase efficiency.\n",
    "\n",
    "**Anomaly Detection and Facial Recognition:** Autoencoders detect anomalies by comparing reconstruction loss to a normal example.\n",
    "\n",
    "**Image and Audio Denoising:** Denoising autoencoders remove extraneous artifacts or corruption in data.\n",
    "\n",
    "**Image Reconstruction:** Autoencoders can fill in missing elements or colorize images.\n",
    "\n",
    "**Generative Tasks:** VAEs and AAEs have been successful in generative tasks, including image and molecular structure generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <b>Datacamp: Introduction to Autoencoders</b>\n",
    "\n",
    "https://www.datacamp.com/tutorial/introduction-to-autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Autoencoder Architecture](../images/autoencoder_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <b>Nima - Machines Learn to Infer Stellar Parameters</b>\n",
    "\n",
    "https://arxiv.org/abs/2009.12872\n",
    "\n",
    "- goal was to put data to low dimension and try to reconstruct, turns out it learned things from raw numbers\n",
    "- note that spectra (signals) is 1-dimensional, images is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Deterministic Convolutional Autoencoder</u>\n",
    "\n",
    "**Architecture**\n",
    "  - Combination of convolutional, up-convolutional, and fully connected layers.\n",
    "  - 15 convolutional layers in the encoder part.\n",
    "  - Bottleneck transforms vectors down to 512 vectors of length 20.\n",
    "  - Code size chosen based on desired compression rate.\n",
    "\n",
    "**Reconstruction Loss**\n",
    "  - Minimized per-pixel L1 loss function for pixel-level accuracy in reconstructed spectrum.\n",
    "  - Empirically computed $L_{AE}$ for reconstruction error.\n",
    "\n",
    "**Median Normalization**\n",
    "  - Normalization of input spectra for stability of training process.\n",
    "  - Original input spectra normalized to ensure consistent value ranges.\n",
    "\n",
    "**Learning Disentangled Representations**\n",
    "  - Key to interpretability.\n",
    "  - Traditional methods of enforcing disentanglement built on VAE-based methods. (variational autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Dataset</u>\n",
    "\n",
    "**HARPS Instrument**\n",
    "  - Dataset built from observations using HARPS instrument.\n",
    "  - Resolving power of 115,000, covering spectral range 378–691nm.\n",
    "  - ∼270,000 HARPS fully reduced spectra used in investigations.\n",
    "\n",
    "**Imbalanced Observations**\n",
    "  - Visibility balancing technique incorporated during training to handle dataset imbalances.\n",
    "  - Parallel experiments conducted to compare results with and without considering dataset imbalances.\n",
    "  - Unique list of objects extracted to ensure each object is looked at only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Reconstruction Results</u>\n",
    "\n",
    "**Deterministic AutoEncoder**\n",
    "  - Quality of reconstructed spectra dependent on bottleneck size.\n",
    "  - Reconstructions displayed with various bottleneck sizes.\n",
    "  - Higher bottleneck dimensions result in more accurate reconstruction of fine features.\n",
    "\n",
    "**With Disentangled Features**\n",
    "  - Disentanglement affects reconstruction quality\n",
    "  - requires more bottleneck dimensions for high-quality reconstruction and disentanglement simultaneously\n",
    "\n",
    "**Training Set vs. Validation Set**\n",
    "  - HARPS dataset split into training and validation subsets for monitoring training process and avoiding overfitting.\n",
    "  - No meaningful difference in reconstruction quality observed across subsets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
