First example - VAE for mnist dataset
Second example - pytorch-mnist-VAE



Can you explain the difference between the two implementations?

    The difference between the architectures of the two examples is that the first example uses
    leaky ReLU activation functions, meaning that it can handle nonlinearity in the dataset.
    The second example only uses fully connected layers (linear), which means the dimensions
    are preserved throughout the forward passing. Another difference is their reparameterization
    process on the encoder outputs before passing to the decoder. Their processes are similar,
    though the logarithmic variance varies due to their abilities of handling nonlinearity. The
    first example, which handles nonlinearity, passes the variance directly to torch.randn_like()
    before reparametizing. The second example, which doesn't handle nonlinearity, calculates the
    standard deviation by raising the encoder outputs to half the variance. It then passes that
    as the argument into torch.randn_like() to reparameterize the encoder outputs. Besides that,
    their implementations of the random-like matrix on the encoder outputs are the same, taking
    the sum of the product of the random matrix & epsilon with the encoder output mean.
    
    First example architecture
    - pass inputs into sequential encoder
        - steps (cycled x2)
            - linear activation function (fully connected layer)
            - then leaky ReLU activation function - max(0.01*x , x)
        - calculate latent mean and logarithmic variance
    - reparameterize encoder's outputs
    - pass reparametized encoder outputs into sequential decoder
        - linear activation function
        - then leaky ReLU activation function
        - then both again, then one more linear
        - sigmoid function
            - adds nonlinearity
            - decides which values are passed as output
    - finally, return final outputs, and encoder mean/variance


    Second example architecture
    - encoder layer
        - 3 fully connected layers
        - calculate latent mean and logarithmic variance (generated by pass thru 3rd layer)
    - similar random sampling to first example reparameterization
    - decoder layer
        - 3 fully connected layers
        - 1 final sigmoid layer
    - finally, return final outputs, and encoder mean/variance


    Similarly, both examples use the same optimizer and loss functions
    - reparameterization layer
        - generate a random like matrix of same dimensions as the input parameter
            - mean of 0
            - variance of 1
        - standard deviation (epsilon): matrix of outputs raised to half the variance
        - multiply matrix by epsilon and add encoder mean
    - optimizer: Adam
        - stochastic optimization
        - adaptive learning rate based on previous iterations
    - custom loss function: 
        - reconstruction error + KL divergence losses
        - binary cross entropy
            - measure the difference between predicted binary outcomes and actual binary labels
            - compares difference between probability distributions
            - penalizes inaccurate predictions
        - Kullbackâ€“Leibler divergence loss
            - measure of how one probability distribution differs from a second



Can you explain the x,y axes of the visualization plot in the first notebook?

    - need to fix it for the first example, it's tracking the losses wrong
    - was tracking the total loss for a single epoch rather than the losses per batch per epoch
    - adjusted to track loss count inside batch loop and appending average to general loss list
    - with adjustments, examples show similar training performance