{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ebaca\\\\Desktop\\\\CMD Research\\\\CMD Remote Repository\\\\autoencoder practice'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing packages & necessary functions'''\n",
    "\n",
    "# pytorch functionalities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# data processing\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# plotting & visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# others\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transofrm to apply to each datapoint\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# download the MNIST datasets\n",
    "path = '../../Local Data Files/MNIST'\n",
    "train_dataset = MNIST(path, transform=transform, download=True)\n",
    "test_dataset  = MNIST(path, transform=transform, download=True)\n",
    "\n",
    "# create train and test dataloaders\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space\n",
    "- compression of input\n",
    "- efficient way of describing data numerically\n",
    "\n",
    "input $\\rarr$ encoder: compression to latent space $\\rarr$ training $\\rarr$ decoder: reconstruct to original dimensions $\\rarr$ output\n",
    "\n",
    "encoder space\n",
    "- number, type, dimension of layers\n",
    "- latent space dimension\n",
    "- want highest compression without losing information\n",
    "\n",
    "decoder space\n",
    "- encoder in reverse (almost)\n",
    "- antonyms of encoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders compress data into a fixed (linear) latent space, which can cause data points of a group to vary (nonlinear) after compression. When mapping these numerical representations, that means that there can be outliers from a group, which prevents an autoencoder from generating an output that can also map back to the same group. This is useful for cases where you want to preserve more information by keeping everything unnormalized, like making exact (or almost) predictions.\n",
    "\n",
    "Variational autoencoders are similar, except they normalize the latent space to remove outliers. This is useful for cases that the autoencoder is prevented from doing, such as generating outputs. \n",
    "\n",
    "The difference between them is the output of the encoder - in an autoencoder, the encoder layer outputs the compressed data, while a the encoder layer of a VAE outputs the mean $\\mu$ and the standard deviation $\\sigma$ of each latent variable, (?)which is each element of a normalized latent vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize & train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### initialize & set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### begin training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
